# DeepResearch 本地部署说明

## ✅ 已完成的配置

1. **创建了 `.env` 配置文件**
   - 模型路径：`Alibaba-NLP/Tongyi-DeepResearch-30B-A3B`
   - 数据集：使用示例数据 `example.jsonl`
   - 输出路径：`./outputs`
   - GPU配置：已修改为使用4个GPU

2. **修改了启动脚本 `run_react_infer.sh`**
   - 从8个GPU改为4个GPU
   - 使用端口 6001-6004

## 🚀 接下来的步骤

### 方法1：使用自动启动脚本（推荐）

```bash
cd /home/intern/jzwww/DeepResearch/inference
bash run_react_infer.sh
```

### 方法2：手动启动（更灵活）

#### 步骤1：启动 vLLM 服务

```bash
# 启动4个服务实例
cd /home/intern/jzwww/DeepResearch

CUDA_VISIBLE_DEVICES=0 vllm serve Alibaba-NLP/Tongyi-DeepResearch-30B-A3B --host 0.0.0.0 --port 6001 --disable-log-requests > vllm_port1.log 2>&1 &
CUDA_VISIBLE_DEVICES=1 vllm serve Alibaba-NLP/Tongyi-DeepResearch-30B-A3B --host 0.0.0.0 --port 6002 --disable-log-requests > vllm_port2.log 2>&1 &
CUDA_VISIBLE_DEVICES=2 vllm serve Alibaba-NLP/Tongyi-DeepResearch-30B-A3B --host 0.0.0.0 --port 6003 --disable-log-requests > vllm_port3.log 2>&1 &
CUDA_VISIBLE_DEVICES=3 vllm serve Alibaba-NLP/Tongyi-DeepResearch-30B-A3B --host 0.0.0.0 --port 6004 --disable-log-requests > vllm_port4.log 2>&1 &

echo "等待服务启动..."
sleep 60

# 验证服务
curl http://localhost:6001/v1/models
```

#### 步骤2：运行推理

```bash
cd /home/intern/jzwww/DeepResearch/inference

python run_multi_react.py \
  --dataset ../inference/eval_data/example.jsonl \
  --output ./outputs \
  --model Alibaba-NLP/Tongyi-DeepResearch-30B-A3B \
  --max_workers 4 \
  --temperature 0.6 \
  --presence_penalty 1.1 \
  --roll_out_count 1
```

## 📝 配置文件说明

### `.env` 文件位置
`/home/intern/jzwww/DeepResearch/.env`

### 当前配置
- **MODEL_PATH**: `Alibaba-NLP/Tongyi-DeepResearch-30B-A3B`（Hugging Face 模型ID）
- **DATASET**: `./inference/eval_data/example.jsonl`
- **OUTPUT_PATH**: `./outputs`
- **MAX_WORKERS**: 4（并行处理数）
- **ROLLOUT_COUNT**: 1（推理轮数）

### 如何修改配置

如果你有更多GPU，可以增加启动的服务数：

1. 编辑 `.env` 文件：
```bash
nano /home/intern/jzwww/DeepResearch/.env
```

2. 修改 MAX_WORKERS 和增加GPU：

编辑 `run_react_infer.sh`，添加更多服务：
```bash
CUDA_VISIBLE_DEVICES=4 vllm serve $MODEL_PATH --host 0.0.0.0 --port 6005 --disable-log-requests &
# ... 继续添加更多
```

并在 `main_ports=(6001 6002 6003 6004)` 中添加更多端口。

## 🔍 检查状态

### 查看 vLLM 服务状态
```bash
# 查看端口占用
netstat -tulpn | grep 600

# 查看服务日志
tail -f /home/intern/jzwww/DeepResearch/inference/vllm_port1.log
```

### 检查输出结果
```bash
# 查看推理结果
ls -lh /home/intern/jzwww/DeepResearch/outputs/

# 查看具体结果内容
cat /home/intern/jzwww/DeepResearch/outputs/Tongyi-DeepResearch-30B-A3B_sglang/example/iter1.jsonl
```

## ⚠️ 注意事项

1. **模型加载**：第一次启动 vLLM 时，模型会从 Hugging Face 缓存加载，需要一些时间
2. **显存要求**：30B-A3B 模型每个实例大约需要 60-80GB 显存
3. **依赖安装**：确保已安装 vLLM：
   ```bash
   pip install vllm
   ```

## 🎯 下一步

1. 检查是否需要工具API（搜索、网页访问等）
2. 准备自己的测试数据集
3. 查看生成的结果

## 📚 相关文档

- 项目 README: `/home/intern/jzwww/DeepResearch/README.md`
- GitHub: https://github.com/Alibaba-NLP/DeepResearch

